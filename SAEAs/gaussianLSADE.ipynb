{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from itertools import product\n",
    "from scipy.stats import qmc  # For Latin Hypercube Sampling\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from PIL import Image\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, meanPrior):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        if meanPrior == 'max':\n",
    "            # self.mean_module = gpytorch.means.ZeroMean()\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            # self.mean_module.constant = torch.nn.Parameter(torch.tensor(torch.max(train_y)))\n",
    "            self.mean_module.constant.data = torch.tensor(torch.max(train_y))\n",
    "\n",
    "        else:\n",
    "            # self.mean_module = gpytorch.means.ConstantMean(constant_prior=torch.max(train_y))\n",
    "            self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "def GPTrain(features, targets, meanPrior):\n",
    "\n",
    "    tensorSamplesXY = torch.from_numpy(features)\n",
    "    tensorSamplesZ = torch.from_numpy(targets)\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood() \n",
    "    model = ExactGPModel(tensorSamplesXY, tensorSamplesZ, likelihood, meanPrior)\n",
    "    likelihood.noise = 1e-4\n",
    "    likelihood.noise_covar.raw_noise.requires_grad_(False)\n",
    "\n",
    "    training_iter = 250\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(tensorSamplesXY)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, tensorSamplesZ)\n",
    "        loss.backward()\n",
    "        # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        #     i + 1, training_iter, loss.item(),\n",
    "        #     model.covar_module.base_kernel.lengthscale.item(), #.kernels[0] after base_kernel if have multiple kernels\n",
    "        #     model.likelihood.noise.item()\n",
    "        # ))\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def GPEval(model, newFeatures):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = model(torch.from_numpy(newFeatures))\n",
    "\n",
    "    mean_pred = observed_pred.mean.numpy()\n",
    "\n",
    "    return mean_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialEvolution:\n",
    "    def __init__(self, bounds, objective_function, pop_size=50, mutation_factor=0.8, crossover_prob=0.7, max_generations=200, method='random'):\n",
    "        \"\"\"\n",
    "        Initialize the Differential Evolution (DE) optimizer.\n",
    "        \n",
    "        Parameters:\n",
    "        bounds (list of tuple): List of (min, max) bounds for each dimension.\n",
    "        pop_size (int): Number of candidate solutions in the population.\n",
    "        mutation_factor (float): Scaling factor for mutation [0, 2].\n",
    "        crossover_prob (float): Crossover probability [0, 1].\n",
    "        max_generations (int): Maximum number of generations to evolve.\n",
    "        method (str): Population initialization method ('random' or 'lhs').\n",
    "        \"\"\"\n",
    "        self.bounds = np.array(bounds)\n",
    "        self.dimensions = len(bounds)\n",
    "        self.pop_size = pop_size\n",
    "        self.mutation_factor = mutation_factor\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.max_generations = max_generations\n",
    "        self.method = method\n",
    "        \n",
    "        # Initialize population\n",
    "        self.population = self.initialize_population()\n",
    "        self.best_solution = None\n",
    "        self.best_fitness = np.inf\n",
    "        self.objective_function = objective_function\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Initialize population using random sampling or Latin Hypercube Sampling.\"\"\"\n",
    "        if self.method == 'lhs':\n",
    "            # Latin Hypercube Sampling\n",
    "            sampler = qmc.LatinHypercube(d=self.dimensions)\n",
    "            sample = sampler.random(n=self.pop_size)\n",
    "            population = qmc.scale(sample, self.bounds[:, 0], self.bounds[:, 1])\n",
    "        else:\n",
    "            # Random Sampling\n",
    "            population = np.random.rand(self.pop_size, self.dimensions)\n",
    "            for i in range(self.dimensions):\n",
    "                population[:, i] = self.bounds[i, 0] + population[:, i] * (self.bounds[i, 1] - self.bounds[i, 0])\n",
    "        \n",
    "        # print(population.shape)\n",
    "        return population\n",
    "    \n",
    "    def mutate(self, target_idx):\n",
    "        \"\"\"Mutation using DE/best/1 strategy.\"\"\"\n",
    "        # Choose three random and distinct individuals different from target_idx\n",
    "        indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n",
    "        np.random.shuffle(indices)\n",
    "        r1, r2 , r3= indices[:3]\n",
    "        \n",
    "        # Best individual in current population\n",
    "\n",
    "        # print(self.population.shape)\n",
    "\n",
    "        #TODO  instead of this list comprehension bollocks just evaluate them all at once\n",
    "        #as thats what i think it wants, then find the minimum of the results. \n",
    "\n",
    "\n",
    "        predictedValues = GPEval(self.objective_function, self.population)\n",
    "\n",
    "        best_idx = np.argsort(predictedValues)[:1]\n",
    "\n",
    "        best = self.population[best_idx]\n",
    "\n",
    "        # best_idx = np.argmin([self.objective_function.predict(ind) for ind in self.population])\n",
    "        # best = self.population[best_idx]\n",
    "        \n",
    "        # Mutant vector: v = best + F * (r1 - r2)\n",
    "        mutant = best + self.mutation_factor * (self.population[r1] - self.population[r2])\n",
    "        \n",
    "        # Ensure mutant vector is within bounds\n",
    "        mutant = np.clip(mutant, self.bounds[:, 0], self.bounds[:, 1])\n",
    "        \n",
    "        return mutant\n",
    "    \n",
    "    def crossover(self, target, mutant):\n",
    "        \"\"\"Crossover to create trial vector.\"\"\"\n",
    "        trial = np.copy(target)\n",
    "        # print(trial.shape)\n",
    "        # print(mutant.shape)\n",
    "        for i in range(self.dimensions):\n",
    "            if np.random.rand() < self.crossover_prob or i == np.random.randint(self.dimensions):\n",
    "                # print(trial[i], mutant[i])\n",
    "                trial[i] = mutant[i]\n",
    "        return trial\n",
    "    \n",
    "    def select(self, target, trial):\n",
    "        \"\"\"Selection: Return the individual with the better fitness.\"\"\"\n",
    "        if self.objective_function.predict(trial) < self.objective_function.predict(target):\n",
    "            return trial\n",
    "        return target\n",
    "\n",
    "    def select(self, target, trial):\n",
    "        \"\"\"Selection: Return the individual with the better fitness.\"\"\"\n",
    "        if GPEval(self.objective_function, trial) < GPEval(self.objective_function, target):\n",
    "            return trial\n",
    "        return target\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"Run the Differential Evolution optimization.\"\"\"\n",
    "        # x_range = np.linspace(-5, 5, 100)\n",
    "        # y_range = np.linspace(-5, 5, 100)\n",
    "        # X, Y = np.meshgrid(x_range, y_range)\n",
    "        # Z = ackley_function(X, Y)\n",
    "        x_range = np.linspace(self.bounds[0,0], self.bounds[0,1],50)\n",
    "        y_range = np.linspace(self.bounds[1,0], self.bounds[1,1],50)\n",
    "        fullRange = list(product(x_range, y_range))\n",
    "        fullRangeArray = np.array(fullRange)\n",
    "        # y_pred = self.objective_function.predict(fullRangeArray)\n",
    "        y_pred = GPEval(self.objective_function, fullRangeArray)\n",
    "\n",
    "        for generation in range(self.max_generations):\n",
    "            new_population = np.zeros_like(self.population)\n",
    "            allTrials = np.zeros_like(self.population)\n",
    "            allTargets = np.zeros_like(self.population)\n",
    "            # print(self.population.shape)\n",
    "            for i in range(self.pop_size):\n",
    "                target = self.population[i]\n",
    "                # print('break')\n",
    "                # print(i)\n",
    "                mutant = self.mutate(i)\n",
    "                # print(mutant)\n",
    "                mutant = np.reshape(mutant, (2,))\n",
    "                # print(mutant)\n",
    "                trial = self.crossover(target, mutant)\n",
    "                trial = np.reshape(trial, (1,-1))\n",
    "                target = np.reshape(target, (1,-1))\n",
    "                # print('for select', trial.shape, target.shape)\n",
    "                new_population[i] = self.select(target, trial)\n",
    "            \n",
    "            # Update the population\n",
    "            self.population = new_population\n",
    "            \n",
    "            # Track the best solution\n",
    "            # best_idx = np.argmin([self.objective_function.predict(ind) for ind in self.population])\n",
    "            # best_fitness = self.objective_function.predict(self.population[best_idx])\n",
    "            \n",
    "            # predictedValues = self.objective_function.predict(self.population)\n",
    "            predictedValues = GPEval(self.objective_function, self.population)\n",
    "\n",
    "            best_idx = np.argsort(predictedValues)[:1]\n",
    "\n",
    "            # best_fitness = self.objective_function.predict(self.population[best_idx])\n",
    "            best_fitness = GPEval(self.objective_function, self.population[best_idx])\n",
    "\n",
    "            if best_fitness < self.best_fitness:\n",
    "                self.best_fitness = best_fitness\n",
    "                self.best_solution = self.population[best_idx]\n",
    "            \n",
    "            # plt.contourf(x_range, y_range, y_pred, levels=50, cmap='viridis')\n",
    "        plt.scatter(fullRangeArray[:,0], fullRangeArray[:,1], c = y_pred)\n",
    "\n",
    "        plt.scatter(self.population[:, 0], self.population[:, 1], color='red', label='Final Population', s=5)\n",
    "        # plt.scatter(best_solution[0], best_solution[1], color='blue', label='Best Solution', s=100)\n",
    "        # plt.legend()\n",
    "        plt.title(\"Local Surrogate\")\n",
    "        plt.colorbar()\n",
    "        plt.clim(0,14)\n",
    "        plt.xlim(self.bounds[0,0], self.bounds[0,1])\n",
    "        plt.ylim(self.bounds[1,0], self.bounds[1,1])\n",
    "        plt.savefig('localGP.png')\n",
    "        plt.close()\n",
    "        # # Debug information\n",
    "        print(f\"Generation {generation + 1}: Best RBF Fitness = {self.best_fitness}\")\n",
    "        \n",
    "        return self.best_solution, self.best_fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackley_function(x, y, a=20, b=0.2, c=2 * np.pi):\n",
    "    term1 = -a * np.exp(-b * np.sqrt(0.5 * (x**2 + y**2)))\n",
    "    term2 = -np.exp(0.5 * (np.cos(c * x) + np.cos(c * y)))\n",
    "    return term1 + term2 + a + np.exp(1)\n",
    "\n",
    "# def ackley_function(x, y, a=20, b=0.2, c=2 * np.pi):\n",
    "\n",
    "#     z = np.sin(x)+(x*np.cos(0.5*y))\n",
    "#     return z\n",
    "\n",
    "def lipschitz_global_underestimate(f_values, samplesXY, L, test_points):\n",
    "    \"\"\"\n",
    "    Compute the Lipschitz-based global underestimate at specific points.\n",
    "\n",
    "    Parameters:\n",
    "    f_values: np.ndarray of shape (n_samples,)\n",
    "        The function values at the sampled points.\n",
    "    samplesXY: np.ndarray of shape (n_samples, 2)\n",
    "        Array of the sampled (x, y) points.\n",
    "    L: float\n",
    "        The Lipschitz constant.\n",
    "    test_points: np.ndarray of shape (n, 2)\n",
    "        Array of (x, y) points at which to compute the underestimate.\n",
    "\n",
    "    Returns:\n",
    "    Z_under: np.ndarray of shape (n,)\n",
    "        The global Lipschitz-based underestimate values at the test points.\n",
    "    \"\"\"\n",
    "    n_test_points = test_points.shape[0]\n",
    "    Z_under = np.full(n_test_points, -np.inf)  # Initialize with very low values\n",
    "\n",
    "    # Loop over all sample points to compute their individual underestimates\n",
    "    for i, (x_i, y_i) in enumerate(samplesXY):\n",
    "        f_x_i_y_i = f_values[i]\n",
    "        \n",
    "        # Compute the distance from each test point to the sample point (x_i, y_i)\n",
    "        distances = np.sqrt((test_points[:, 0] - x_i)**2 + (test_points[:, 1] - y_i)**2)\n",
    "        \n",
    "        # Compute the local Lipschitz underestimate for this sample\n",
    "        Z_local_under = f_x_i_y_i - L * distances\n",
    "        \n",
    "        # Update the global underestimate by taking the maximum across samples\n",
    "        Z_under = np.maximum(Z_under, Z_local_under)\n",
    "    \n",
    "    return Z_under\n",
    "\n",
    "def estimate_lipschitz_constant(samplesXY, f_values):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimate the Lipschitz constant based on known sample points and their function values.\n",
    "    \n",
    "    Parameters:\n",
    "    samplesXY: np.ndarray of shape (n_samples, 2)\n",
    "        Array of the sampled (x, y) points.\n",
    "    f_values: np.ndarray of shape (n_samples,)\n",
    "        Array of function values at the sampled points.\n",
    "    \n",
    "    Returns:\n",
    "    L_est: float\n",
    "        The estimated Lipschitz constant.\n",
    "    \"\"\"\n",
    "    n_samples = samplesXY.shape[0]\n",
    "    # print(samplesXY.shape)\n",
    "    # print(n_samples)\n",
    "    L_est = 0.0\n",
    "    \n",
    "    # Loop over all pairs of points to estimate L\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            # Compute the Euclidean distance between points i and j\n",
    "            dist = np.linalg.norm(samplesXY[i] - samplesXY[j])\n",
    "            if dist > 0:\n",
    "                # Compute the absolute difference in function values\n",
    "                f_diff = np.abs(f_values[i] - f_values[j])\n",
    "                # Compute the slope and update the maximum\n",
    "                # print(L_est, f_diff/dist)\n",
    "                L_est = max(L_est, f_diff / dist)\n",
    "    \n",
    "    return L_est\n",
    "\n",
    "class RBFSurrogateModel:\n",
    "    def __init__(self, epsilon=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the RBF Surrogate Model.\n",
    "\n",
    "        Parameters:\n",
    "        epsilon: float\n",
    "            The shape parameter for the Gaussian RBF kernel. It controls the width of the Gaussian basis function.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "\n",
    "    def _rbf_kernel(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the RBF (Gaussian) kernel between two sets of data points.\n",
    "\n",
    "        Parameters:\n",
    "        X1: ndarray of shape (n_samples_X1, n_features)\n",
    "            First set of data points.\n",
    "        X2: ndarray of shape (n_samples_X2, n_features)\n",
    "            Second set of data points.\n",
    "        \n",
    "        Returns:\n",
    "        K: ndarray of shape (n_samples_X1, n_samples_X2)\n",
    "            Kernel matrix.\n",
    "        \"\"\"\n",
    "        # Compute the pairwise Euclidean distance between points in X1 and X2\n",
    "        distances = cdist(X1, X2, 'euclidean')\n",
    "        # Gaussian RBF: exp(-epsilon^2 * distance^2)\n",
    "        K = np.exp(-(self.epsilon * distances) ** 2)\n",
    "        return K\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the RBF surrogate model on the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X_train: ndarray of shape (n_samples, n_features)\n",
    "            Training data inputs.\n",
    "        y_train: ndarray of shape (n_samples,)\n",
    "            Training data targets.\n",
    "        \"\"\"\n",
    "        # print('xtrain', X_train.shape)\n",
    "        self.centers = X_train  # The centers of the RBF are the training data points\n",
    "        # Compute the RBF kernel matrix for the training data\n",
    "        K_train = self._rbf_kernel(X_train, X_train)\n",
    "        # Solve the linear system K * weights = y_train to get the weights\n",
    "        self.weights = np.linalg.solve(K_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Make predictions for new input data using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X_test: ndarray of shape (n_samples_test, n_features)\n",
    "            New data inputs for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "        y_pred: ndarray of shape (n_samples_test,)\n",
    "            Predicted values for the input data.\n",
    "        \"\"\"\n",
    "        # Compute the RBF kernel between the test data and the centers\n",
    "        K_test = self._rbf_kernel(X_test, self.centers)\n",
    "        # Multiply the kernel matrix by the weights to get predictions\n",
    "        #apparently @ is python syntax for matrix multiplication...\n",
    "        y_pred = K_test @ self.weights\n",
    "        return y_pred\n",
    "    \n",
    "def objective_function(vec):\n",
    "    \"\"\"Objective function wrapper for optimization.\n",
    "    Args:\n",
    "        vec (np.ndarray): A vector representing candidate solution (x, y).\n",
    "    Returns:\n",
    "        float: Fitness value of the solution.\n",
    "    \"\"\"\n",
    "    x, y = vec\n",
    "    return ackley_function(x, y)\n",
    "\n",
    "# Differential Evolution Optimizer\n",
    "class LSADE:\n",
    "    def __init__(self, bounds, pop_size=50, mutation_factor=0.8, crossover_prob=0.7, method='random'):\n",
    "        \"\"\"\n",
    "        Initialize the Differential Evolution (DE) optimizer.\n",
    "        \n",
    "        Parameters:\n",
    "        bounds (list of tuple): List of (min, max) bounds for each dimension.\n",
    "        pop_size (int): Number of candidate solutions in the population.\n",
    "        mutation_factor (float): Scaling factor for mutation [0, 2].\n",
    "        crossover_prob (float): Crossover probability [0, 1].\n",
    "        max_generations (int): Maximum number of generations to evolve.\n",
    "        method (str): Population initialization method ('random' or 'lhs').\n",
    "        \"\"\"\n",
    "        self.bounds = np.array(bounds)\n",
    "        self.dimensions = len(bounds)\n",
    "        self.pop_size = pop_size\n",
    "        self.mutation_factor = mutation_factor\n",
    "        self.crossover_prob = crossover_prob\n",
    "        # self.max_generations = max_generations\n",
    "        self.method = method\n",
    "        self.feFeatures = np.empty((0,2))\n",
    "        self.feTargets = np.empty(0)        \n",
    "        # Initialize population\n",
    "        self.population = self.initialize_population()\n",
    "\n",
    "        self.best_solution = None\n",
    "        self.best_fitness = np.inf\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Initialize population using random sampling or Latin Hypercube Sampling.\"\"\"\n",
    "\n",
    "        # print('initial shape', self.feFeatures.shape)\n",
    "        if self.method == 'lhs':\n",
    "            # Latin Hypercube Sampling\n",
    "            sampler = qmc.LatinHypercube(d=self.dimensions)\n",
    "            sample = sampler.random(n=self.pop_size)\n",
    "            population = qmc.scale(sample, self.bounds[:, 0], self.bounds[:, 1])\n",
    "        else:\n",
    "            # Random Sampling\n",
    "            population = np.random.rand(self.pop_size, self.dimensions)\n",
    "            for i in range(self.dimensions):\n",
    "                population[:, i] = self.bounds[i, 0] + population[:, i] * (self.bounds[i, 1] - self.bounds[i, 0])\n",
    "\n",
    "        for i in range(0, len(population)):\n",
    "            self.feTargets = np.append(self.feTargets, objective_function(population[i]))\n",
    "            self.feFeatures = np.vstack((self.feFeatures, population[i]))\n",
    "        \n",
    "        # print('final shape', self.feFeatures.shape)\n",
    "\n",
    "        return population\n",
    "    \n",
    "    def mutate(self, target_idx):\n",
    "        \"\"\"Mutation using DE/best/1 strategy.\"\"\"\n",
    "        # Choose three random and distinct individuals different from target_idx\n",
    "        indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n",
    "        np.random.shuffle(indices)\n",
    "        r1, r2 , r3= indices[:3]\n",
    "        \n",
    "        # Best individual in current population\n",
    "        best_idx = np.argmin([objective_function(ind) for ind in self.population])\n",
    "        best = self.population[best_idx]\n",
    "        \n",
    "        # Mutant vector: v = best + F * (r1 - r2)\n",
    "        mutant = best + self.mutation_factor * (self.population[r1] - self.population[r2])\n",
    "        \n",
    "        # Ensure mutant vector is within bounds\n",
    "        mutant = np.clip(mutant, self.bounds[:, 0], self.bounds[:, 1])\n",
    "        \n",
    "        return mutant\n",
    "    \n",
    "    def crossover(self, target, mutant):\n",
    "        \"\"\"Crossover to create trial vector.\n",
    "        This loops through the features in the vector (dimensions) and sees if any of them crossover. \n",
    "        allows the retention of some original vector features but not others. \n",
    "        \"\"\"\n",
    "\n",
    "        trial = np.copy(target)\n",
    "        # print(trial.shape)\n",
    "        # print(mutant.shape)\n",
    "        for i in range(self.dimensions):\n",
    "            if np.random.rand() < self.crossover_prob or i == np.random.randint(self.dimensions):\n",
    "                # print(trial[i], mutant[i])\n",
    "\n",
    "                trial[i] = mutant[i]\n",
    "        return trial\n",
    "            \n",
    "\n",
    "    # def select(self, target, trial):\n",
    "    #     \"\"\"Selection: Return the individual with the better fitness.\"\"\"\n",
    "    #     if objective_function(trial) < objective_function(target):\n",
    "    #         return trial\n",
    "    #     return target\n",
    "    \n",
    "    def localRBF(self, numSolutions):\n",
    "\n",
    "        bestFeatures = np.empty((numSolutions,2))\n",
    "        bestTargets = np.empty(numSolutions)\n",
    "\n",
    "        #find c best solutions\n",
    "        bestIndices = np.argsort(self.feTargets)[:numSolutions]\n",
    "\n",
    "        for i in range(numSolutions):\n",
    "            bestFeatures[i] = self.feFeatures[bestIndices[i]]\n",
    "            bestTargets[i] = self.feTargets[bestIndices[i]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x_min, x_max = np.min(bestFeatures[:, 0]), np.max(bestFeatures[:, 0])\n",
    "        y_min, y_max = np.min(bestFeatures[:, 1]), np.max(bestFeatures[:, 1])\n",
    "\n",
    "        bounds = [(x_min, x_max), (y_min, y_max)]\n",
    "\n",
    "        # pairwiseDistancesLocal = np.linalg.norm(bestFeatures[:, np.newaxis] - bestFeatures, axis=2)\n",
    "        # avgDistanceLocal = np.mean(pairwiseDistancesLocal)\n",
    "\n",
    "        localGP = GPTrain(bestFeatures, bestTargets, meanPrior='max')\n",
    "\n",
    "        # localRBF = RBFSurrogateModel(epsilon=1.0)\n",
    "        # localRBF.fit(bestFeatures, bestTargets)\n",
    "\n",
    "        # functionEval = localRBF.predict()\n",
    "        localDE = DifferentialEvolution(bounds, localGP )\n",
    "        bestLocalSolution, bestLocalFitness = localDE.optimize()\n",
    "\n",
    "        return bestLocalSolution\n",
    "    \n",
    "    def optimizerStep(self):\n",
    "        \"\"\"Run the Differential Evolution optimization.\"\"\"\n",
    "        # x_range = np.linspace(-5, 5, 100)\n",
    "        # y_range = np.linspace(-5, 5, 100)\n",
    "        # X, Y = np.meshgrid(x_range, y_range)\n",
    "        # Z = ackley_function(X, Y)\n",
    "\n",
    "        # for generation in range(self.max_generations):\n",
    "        new_population = np.zeros_like(self.population)\n",
    "            \n",
    "        for i in range(self.pop_size):\n",
    "            # print(i)\n",
    "            target = self.population[i]\n",
    "            mutant = self.mutate(i)\n",
    "            trial = self.crossover(target, mutant)\n",
    "            #do not evaluate target/trial vectors, instead just save the trial vectors as the new population\n",
    "            #so they can be evaluated on the global RBF and Lipschitz surrogates later\n",
    "            # new_population[i] = self.select(target, trial)\n",
    "            new_population[i] = trial\n",
    "            \n",
    "            # Update the population\n",
    "        self.population = new_population\n",
    "        # print(new_population.shape)\n",
    "        # pairwise_distances = np.linalg.norm(self.feFeatures[:,np.newaxis] - self.feFeatures, axis=2)\n",
    "        # avg_distance = np.mean(pairwise_distances)\n",
    "        # globalRBF = RBFSurrogateModel(epsilon=1.0)\n",
    "\n",
    "        # globalRBF.fit(self.feFeatures, self.feTargets)\n",
    "\n",
    "        GPModel = GPTrain(self.feFeatures, self.feTargets, meanPrior='zero')\n",
    "\n",
    "        popOnGP = GPEval(GPModel, self.population)\n",
    "\n",
    "        \n",
    "        #evaluating whole landscape on RBF for plotting reasons:\n",
    "        x_range = np.linspace(-5,5,50)\n",
    "        y_range = np.linspace(-5,5,50)\n",
    "        fullRange = list(product(x_range, y_range))\n",
    "        fullRangeArray = np.array(fullRange)\n",
    "        y_pred = GPEval(GPModel, fullRangeArray)\n",
    "\n",
    "\n",
    "        #evaluate current population (children) on RBF\n",
    "        # popOnRBF = globalRBF.predict(self.population)\n",
    "\n",
    "        #function evaluation of best predicted child\n",
    "        best_idx = np.argmin(popOnGP)\n",
    "\n",
    "        bestFeature = self.population[best_idx]\n",
    "\n",
    "        # print('best index =', best_idx)\n",
    "        #evaluate best child and add results to global stores of FE features and targets\n",
    "        self.feTargets = np.append(self.feTargets, objective_function(self.population[best_idx]))\n",
    "        self.feFeatures = np.vstack((self.feFeatures, self.population[best_idx]))\n",
    "\n",
    "        plt.scatter(fullRangeArray[:,0], fullRangeArray[:,1], c = y_pred)\n",
    "\n",
    "        plt.scatter(self.population[:, 0], self.population[:, 1], color='red', label='Final Population', s=5)\n",
    "        plt.scatter(bestFeature[0], bestFeature[1], color='blue', label='Best Solution', s=10)\n",
    "        # plt.legend()\n",
    "        plt.title(\"Global Surrogate\")\n",
    "        plt.colorbar()\n",
    "        plt.clim(0,14)\n",
    "        plt.savefig('globalGP.png')\n",
    "        plt.close()\n",
    "        #generate Lipschitz surrogate, evaluate children, FE evaluate best potential child and add to bank\n",
    "\n",
    "        # print(self.feTargets, self.feFeatures)\n",
    "        # print('final shape', self.feFeatures.shape)\n",
    "\n",
    "        L_est = estimate_lipschitz_constant(self.feFeatures, self.feTargets)\n",
    "\n",
    "        popOnLipschitz = lipschitz_global_underestimate(self.feTargets, self.feFeatures, L_est, self.population)\n",
    "        best_idx = np.argmin(popOnLipschitz)\n",
    "        bestFeature = self.population[best_idx]\n",
    "\n",
    "        self.feTargets = np.append(self.feTargets, objective_function(self.population[best_idx]))\n",
    "        self.feFeatures = np.vstack((self.feFeatures, self.population[best_idx]))        \n",
    "\n",
    "        #evaluating all points in function on Lipschitz for plotting purposes\n",
    "        Z_under = lipschitz_global_underestimate(self.feTargets, self.feFeatures, L_est, fullRangeArray)\n",
    "\n",
    "        plt.scatter(fullRangeArray[:,0], fullRangeArray[:,1], c = Z_under)\n",
    "\n",
    "        plt.scatter(self.population[:, 0], self.population[:, 1], color='red', label='Final Population', s=5)\n",
    "        plt.scatter(bestFeature[0], bestFeature[1], color='blue', label='Best Solution', s=10)\n",
    "        # plt.legend()\n",
    "        plt.title(\"Lipschitz Underestimation\")\n",
    "        plt.colorbar()\n",
    "        plt.clim(0,14)\n",
    "        plt.savefig('lipschitz.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        #construct local RBF using c best solutions, find minima using DE, and evaluate at that minima\n",
    "\n",
    "        bestLocalSolution = self.localRBF(15)\n",
    "\n",
    "        bestLocalSolution = np.reshape(bestLocalSolution, (2,))\n",
    "\n",
    "        print('best local Solution', bestLocalSolution)\n",
    "\n",
    "        self.feTargets = np.append(self.feTargets, objective_function(bestLocalSolution))\n",
    "        self.feFeatures = np.vstack((self.feFeatures, bestLocalSolution)) \n",
    "\n",
    "        plt.scatter(self.feFeatures[:,0], self.feFeatures[:,1], c = self.feTargets)\n",
    "        plt.title('Evaluated Population')\n",
    "        plt.colorbar()\n",
    "        plt.clim(0,14)\n",
    "        plt.savefig('population.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Track the best solution\n",
    "        # best_idx = np.argmin([objective_function(ind) for ind in self.population])\n",
    "        # best_fitness = objective_function(self.population[best_idx])\n",
    "        \n",
    "        # if best_fitness < self.best_fitness:\n",
    "        #     self.best_fitness = best_fitness\n",
    "        #     self.best_solution = self.population[best_idx]\n",
    "            \n",
    "            # plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "            # plt.scatter(de.population[:, 0], de.population[:, 1], color='red', label='Final Population', s=5)\n",
    "            # # plt.scatter(best_solution[0], best_solution[1], color='blue', label='Best Solution', s=100)\n",
    "            # plt.legend()\n",
    "            # plt.title(\"Ackley Function with Final Population and Best Solution\")\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "            # Debug information\n",
    "            # print(f\"Generation {generation + 1}: Best Fitness = {self.best_fitness}\")\n",
    "        print('Best found solution = ', min(self.feTargets))\n",
    "\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "        localGP = Image.open('localGP.png')\n",
    "        globalGP = Image.open('globalGP.png')\n",
    "        lipschitz = Image.open('lipschitz.png')\n",
    "        population = Image.open('population.png')\n",
    "\n",
    "        width, height = localGP.size\n",
    "        combinedImage = Image.new('RGB', (2 * width, 2 * height))\n",
    "        combinedImage.paste(localGP, (0, 0))\n",
    "        combinedImage.paste(lipschitz, (width, 0))\n",
    "        combinedImage.paste(globalGP, (0, height))\n",
    "        combinedImage.paste(population, (width, height))\n",
    "\n",
    "        combinedImage.save(f'plots/{timestamp}.png')\n",
    "\n",
    "\n",
    "        return self.best_solution, self.best_fitness\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/wlgb17ln00dg_lln7zq_3gpw0000gn/T/ipykernel_3307/4061782729.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean_module.constant.data = torch.tensor(torch.max(train_y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 200: Best RBF Fitness = [4.09392402]\n",
      "best local Solution [-0.00792783  0.3012966 ]\n",
      "Best found solution =  2.146502457252567\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [1.97587113]\n",
      "best local Solution [-0.16703728  0.22014475]\n",
      "Best found solution =  2.076729373844906\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [1.08836919]\n",
      "best local Solution [-0.02088193  0.09772967]\n",
      "Best found solution =  0.5285992056711\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [-0.33731822]\n",
      "best local Solution [ 0.13363946 -0.07007954]\n",
      "Best found solution =  0.38439515728279305\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.2366061]\n",
      "best local Solution [0.00379194 0.00731433]\n",
      "Best found solution =  0.025109510466287066\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [-0.00845021]\n",
      "best local Solution [ 0.00629551 -0.02226622]\n",
      "Best found solution =  0.025109510466287066\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0269633]\n",
      "best local Solution [0.01504517 0.00518854]\n",
      "Best found solution =  0.025109510466287066\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.04084057]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.020042749911376223\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02985351]\n",
      "best local Solution [0.00015322 0.00344452]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01745091]\n",
      "best local Solution [-0.00598094  0.00535128]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02005557]\n",
      "best local Solution [-0.0047884   0.00508381]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02024023]\n",
      "best local Solution [-0.00469409  0.0050454 ]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02035804]\n",
      "best local Solution [-0.0042848   0.00473326]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01994861]\n",
      "best local Solution [-0.00451691  0.00488006]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01982801]\n",
      "best local Solution [-0.00452837  0.00591397]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02019155]\n",
      "best local Solution [-0.00447952  0.00578271]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02035004]\n",
      "best local Solution [-0.00421302  0.00664933]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02036645]\n",
      "best local Solution [-0.00378501  0.00516027]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.02017484]\n",
      "best local Solution [-0.00333247  0.00486634]\n",
      "Best found solution =  0.010068747700942016\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01580206]\n",
      "best local Solution [-9.15486150e-05  2.03299655e-03]\n",
      "Best found solution =  0.005866286313342517\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01349313]\n",
      "best local Solution [0.00024439 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01876859]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01848632]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01906478]\n",
      "best local Solution [0.00258623 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01572472]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01727798]\n",
      "best local Solution [0.00336873 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01445896]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01626085]\n",
      "best local Solution [-0.00164893  0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01383664]\n",
      "best local Solution [-0.00451691  0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01442059]\n",
      "best local Solution [-0.00451691  0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01322703]\n",
      "best local Solution [-0.00451691  0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.01473605]\n",
      "best local Solution [0.00028887 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n",
      "Generation 200: Best RBF Fitness = [0.0128724]\n",
      "best local Solution [0.00656303 0.00117721]\n",
      "Best found solution =  0.0034391262088360186\n",
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "bounds = [(-5,5), (-5,5)] #bounds for each dimension (x and y)\n",
    "\n",
    "#initialise DE solver\n",
    "LSADE = LSADE(bounds=bounds, pop_size=50, method='lhs')\n",
    "\n",
    "iter = 0\n",
    "\n",
    "while iter < 50:\n",
    "    LSADE.optimizerStep()\n",
    "    print(LSADE.population.shape)\n",
    "    iter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "\n",
    "image_files = sorted(glob.glob(\"plots/*.png\"))\n",
    "# print('here')\n",
    "images = [Image.open(img) for img in image_files]\n",
    "\n",
    "# Step 3: Save images as a GIF\n",
    "if images:\n",
    "    # 'duration' is the time each frame stays on screen (milliseconds)\n",
    "    images[0].save(\n",
    "        \"output6.gif\",\n",
    "        save_all=True,\n",
    "        append_images=images[1:],  # Include the rest of the images\n",
    "        duration=400,               # Duration of each frame in milliseconds\n",
    "        loop=0                       # 0 means loop indefinitely\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenFOAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
